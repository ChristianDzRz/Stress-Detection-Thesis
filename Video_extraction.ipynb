{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten, Input, Concatenate\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pathlib\n",
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MMI_emotions =  pathlib.Path(\"C:/Users/cdr03/Documents/Thesis/dataset/MMI/Emotions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_recursive(path, format = \".txt\"):\n",
    "    txt_files = set()  # create an empty set to store unique txt file names\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith(format):\n",
    "                file_path = os.path.join(root, file)\n",
    "                txt_files.add(file_path)  # add the file path to the set\n",
    "        for dir in dirs:\n",
    "            dir_path = os.path.join(root, dir)\n",
    "            list_files_recursive(dir_path)  # recursively call the function on each subdirectory\n",
    "    return list(txt_files)  # return a list of unique txt file names\n",
    "\n",
    "MMI_emotion_videos = list_files_recursive(MMI_emotions, \".avi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_array(img, size):\n",
    "    img_array = np.array(img , dtype=\"float32\")/255\n",
    "    img_array = img_array.reshape(size, size, 1)\n",
    "    # Return the numpy array\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 48\n",
    "def detect_and_crop_face(img,resize=None):\n",
    "    # Load the input image\n",
    "    \t\n",
    "    image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # Load the Haar Cascade classifier for face detection\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "    # Detect faces in the image\n",
    "    faces = face_cascade.detectMultiScale(image, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "    # Crop the image to contain the detected face (if any)\n",
    "    if len(faces) > 0:\n",
    "        (x, y, w, h) = faces[0]\n",
    "        face_image = image[y:y+h, x:x+w]\n",
    "        face_image = cv2.resize(face_image, (size, size))\n",
    "        face_image = image_to_array(face_image, size)\n",
    "    else:\n",
    "        rotated=cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)\n",
    "        rotated_faces = face_cascade.detectMultiScale(rotated, scaleFactor=1.1, minNeighbors=5)\n",
    "        if len(rotated_faces) > 0:\n",
    "            (x, y, w, h) = rotated_faces[0]\n",
    "            face_image = rotated[y:y+h, x:x+w]\n",
    "            face_image = cv2.resize(face_image, (size, size))\n",
    "            face_image = image_to_array(face_image, size)\n",
    "        else:\n",
    "            face_image = None\n",
    "            print(\"face wasn't detected\")\n",
    "    return face_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/blob/master/Chapter08/ch8_nb1_action_recognition.ipynb\n",
    "\n",
    "video_paths = MMI_emotion_videos\n",
    "print(type(MMI_emotion_videos))\n",
    "SEQUENCE_LENGTH = 10\n",
    "IMG_size = (48, 48)\n",
    "def frame_generator():\n",
    "    np.random.shuffle(video_paths)\n",
    "    for video_path in video_paths:\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        sample_every_frame = max(1, num_frames // SEQUENCE_LENGTH)\n",
    "        current_frame = 0\n",
    "\n",
    "        label = os.path.basename(video_path)\n",
    "\n",
    "        max_images = SEQUENCE_LENGTH\n",
    "        while True:\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "\n",
    "            if current_frame % sample_every_frame == 0:\n",
    "                # OPENCV reads in BGR, tensorflow expects RGB so we invert the order\n",
    "                face_image_frame = detect_and_crop_face(frame, size)\n",
    "                if face_image_frame is not None:\n",
    "                    yield face_image_frame, video_path \n",
    "                else:\n",
    "                    pass\n",
    "            if max_images == 0:\n",
    "                break\n",
    "            current_frame += 1\n",
    "\n",
    "# `from_generator` might throw a warning, expected to disappear in upcoming versions:\n",
    "# https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#for_example_2\n",
    "dataset = tf.data.Dataset.from_generator(frame_generator,\n",
    "             output_types=(tf.float32, tf.string),\n",
    "             output_shapes=((48, 48, 1), ()))\n",
    "\n",
    "dataset = dataset.batch(16).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_input_layer(pretrained_model, first_conv, h, w, c):\n",
    "    pretrained_config = pretrained_model.get_config()\n",
    "\n",
    "    # Change the input shape from the model\n",
    "    pretrained_config[\"layers\"][0][\"config\"][\"batch_input_shape\"] = (None, h, w, c)\n",
    "\n",
    "    updated_model = Model.from_config(pretrained_config)\n",
    "\n",
    "\n",
    "    def avg_weights(weights):\n",
    "        average_weights = np.mean(weights, axis=-2).reshape(weights[:, :, -1:, :].shape)\n",
    "        return(average_weights)\n",
    "\n",
    "    pretrained_updated_config = updated_model.get_config()\n",
    "    pretrained_updated_layer_names = [pretrained_updated_config['layers'][x]['name'] for x in range(len(pretrained_updated_config['layers']))]\n",
    "    print(pretrained_updated_layer_names)\n",
    "    first_conv_name = pretrained_updated_layer_names[first_conv]\n",
    "\n",
    "    for layer in pretrained_model.layers:\n",
    "        if layer.name in pretrained_updated_layer_names:\n",
    "            if layer.get_weights() != []:  #All convolutional layers and layers with weights (no input layer or any pool layers)\n",
    "                target_layer = updated_model.get_layer(layer.name)\n",
    "            \n",
    "                if layer.name in first_conv_name:    #For the first convolutionl layer\n",
    "                    weights = layer.get_weights()[0]\n",
    "                    biases  = layer.get_weights()[1]\n",
    "                    \n",
    "                    weights_single_channel = avg_weights(weights)\n",
    "                                                                \n",
    "                    target_layer.set_weights([weights_single_channel, biases])  #Now set weights for the first conv. layer\n",
    "                    target_layer.trainable = False   #You can make this trainable if you want. \n",
    "            \n",
    "                else:\n",
    "                    target_layer.set_weights(layer.get_weights())   #Set weights to all other layers. \n",
    "                    target_layer.trainable = False  #You can make this trainable if you want.\n",
    "    updated_model.summary()\n",
    "    return updated_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_4', 'block1_conv1', 'block1_conv2', 'block1_pool', 'block2_conv1', 'block2_conv2', 'block2_pool', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block3_pool', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block4_pool', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'block5_pool']\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 48, 48, 1)]       0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 48, 48, 64)        640       \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 48, 48, 64)        36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 24, 24, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 24, 24, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 12, 12, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 12, 12, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 12, 12, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 6, 6, 512)         1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 3, 3, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 3, 3, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 3, 3, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 3, 3, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,713,536\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,713,536\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16_model = VGG16(include_top=False, weights='imagenet')\n",
    "vgg16_updated = change_input_layer(vgg16_model, 1, 48, 48, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vgg16 = Sequential()\n",
    "final_vgg16.add(vgg16_updated)\n",
    "final_vgg16.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:30,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face wasn't detected\n",
      "face wasn't detected\n",
      "face wasn't detected\n",
      "face wasn't detected\n",
      "face wasn't detected\n",
      "face wasn't detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [01:30,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face wasn't detected\n",
      "face wasn't detected\n",
      "face wasn't detected\n",
      "face wasn't detected\n",
      "face wasn't detected\n",
      "face wasn't detected\n",
      "face wasn't detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73it [03:00,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face wasn't detected\n",
      "face wasn't detected\n",
      "face wasn't detected\n",
      "face wasn't detected\n",
      "face wasn't detected\n",
      "face wasn't detected\n",
      "face wasn't detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [03:29,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face wasn't detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85it [03:32,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face wasn't detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "161it [07:04,  2.64s/it]\n"
     ]
    }
   ],
   "source": [
    "current_path = None\n",
    "all_features = []\n",
    "files = []\n",
    "\n",
    "for img, batch_paths in tqdm.tqdm(dataset):\n",
    "    batch_features = final_vgg16(img)\n",
    "    batch_features = tf.reshape(batch_features, \n",
    "                              (batch_features.shape[0], -1))\n",
    "    \n",
    "    for features, path in zip(batch_features.numpy(), batch_paths.numpy()):\n",
    "        if path != current_path and current_path is not None:\n",
    "            #output_path = current_path.decode().replace('.avi', '.npy')\n",
    "            #np.save(output_path, all_features)\n",
    "            all_features = []\n",
    "            \n",
    "        current_path = path\n",
    "        all_features.append(features)\n",
    "        files.append(current_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'C:\\\\Users\\\\cdr03\\\\Documents\\\\Thesis\\\\dataset\\\\MMI\\\\Emotions\\\\1962\\\\S048-001.avi'], shape=(1,), dtype=string)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "TypeError: Can't convert object to 'str' for 'filename'\nTraceback (most recent call last):\n\n  File \"c:\\Users\\cdr03\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"c:\\Users\\cdr03\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"c:\\Users\\cdr03\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1004, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\cdr03\\AppData\\Local\\Temp\\ipykernel_34232\\237410190.py\", line 27, in frame_generator\n    face_image_frame = detect_and_crop_face(frame)\n\n  File \"C:\\Users\\cdr03\\AppData\\Local\\Temp\\ipykernel_34232\\3008466000.py\", line 3, in detect_and_crop_face\n    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n\nTypeError: Can't convert object to 'str' for 'filename'\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34232\\2978157326.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\cdr03\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    834\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 836\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    837\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    838\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cdr03\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    817\u001b[0m     \u001b[1;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[0;32m    820\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cdr03\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2920\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2921\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2922\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2923\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2924\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\cdr03\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7184\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7185\u001b[0m   \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7186\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: TypeError: Can't convert object to 'str' for 'filename'\nTraceback (most recent call last):\n\n  File \"c:\\Users\\cdr03\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 271, in __call__\n    ret = func(*args)\n\n  File \"c:\\Users\\cdr03\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"c:\\Users\\cdr03\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 1004, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\cdr03\\AppData\\Local\\Temp\\ipykernel_34232\\237410190.py\", line 27, in frame_generator\n    face_image_frame = detect_and_crop_face(frame)\n\n  File \"C:\\Users\\cdr03\\AppData\\Local\\Temp\\ipykernel_34232\\3008466000.py\", line 3, in detect_and_crop_face\n    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n\nTypeError: Can't convert object to 'str' for 'filename'\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "img, file_name = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
